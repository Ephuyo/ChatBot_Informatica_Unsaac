{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ephuyo/ChatBot_Informatica_Unsaac/blob/main/YachayBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la biblioteca 'unidecode' para realizar la transliteración de caracteres Unicode a caracteres ASCII\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "yj6lcWbPa2Mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878d1ec7-11f5-4f8d-f7a8-090b9575e8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/235.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------Importar las bibliotecas necesarias----------------------------------------------------------------------\n",
        "# Para manejar archivos JSON\n",
        "import json\n",
        "# Biblioteca de procesamiento de lenguaje natural\n",
        "import nltk\n",
        "# Para remover acentos y caracteres especiales de texto\n",
        "import unidecode\n",
        "# Biblioteca de procesamiento de lenguaje natural avanzado\n",
        "import spacy\n",
        "# Función para tokenizar palabras\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Lista de palabras detenidas (stop words)\n",
        "from nltk.corpus import stopwords\n",
        "# Para crear vectores TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Para calcular similitud de coseno\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "zLYuGR9JQZI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar los recursos necesarios de NLTK\n",
        "nltk.download('punkt')  # Descargar el tokenizador de NLTK\n",
        "nltk.download('stopwords')  # Descargar la lista de stop words en español"
      ],
      "metadata": {
        "id": "MJdDPzgdQ9Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ec7506-7f3a-48fa-bcfd-369576baf1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actualiza la biblioteca 'spacy' a la última versión disponible\n",
        "!pip install -U spacy\n",
        "\n",
        "# Descarga el modelo de procesamiento de lenguaje natural en español (\"es_core_news_sm\") de spaCy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "7l9v2Zx_bQIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de lenguaje en español de spaCy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "BZ8Yd-_bRNrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el corpus desde un archivo JSON\n",
        "with open('corpus.json', 'r', encoding='utf-8') as archivo:\n",
        "    corpus = json.load(archivo)"
      ],
      "metadata": {
        "id": "HLXwz-r-Rhcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las palabras detenidas (stop words) en español\n",
        "palabras_detenidas = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "__QOWfc_RkkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesar_texto(texto):\n",
        "    # Procesar el texto con el modelo de spaCy\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Filtrar los tokens basados en ciertas propiedades\n",
        "    tokens_filtrados = [\n",
        "        token.lemma_ for token in doc\n",
        "        if not token.is_punct             # No es un signo de puntuación\n",
        "        and not token.is_space            # No es un espacio en blanco\n",
        "        and not token.is_stop             # No es una palabra de parada (palabra común que se suele filtrar en el procesamiento de lenguaje)\n",
        "        and not token.is_digit            # No es un dígito\n",
        "        and not token.like_num            # No se parece a un número (puede ser una combinación de dígitos y letras)\n",
        "    ]\n",
        "\n",
        "    # Unir los tokens filtrados en una cadena de texto\n",
        "    return ' '.join(tokens_filtrados)\n"
      ],
      "metadata": {
        "id": "_JvzFlZOXzIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_vectores_tfidf(corpus):\n",
        "    # Crear una lista para almacenar todas las respuestas del corpus\n",
        "    todas_las_respuestas = []\n",
        "\n",
        "    # Recorrer cada sección en el corpus y extender la lista con los patrones de respuesta\n",
        "    for seccion in corpus.values():\n",
        "        todas_las_respuestas.extend(seccion['patrones'])\n",
        "\n",
        "    # Crear un vectorizador TF-IDF con el preprocesamiento de texto definido anteriormente\n",
        "    vectorizador = TfidfVectorizer(preprocessor=preprocesar_texto)\n",
        "\n",
        "    # Transformar las respuestas en una matriz TF-IDF utilizando el vectorizador\n",
        "    matriz_tfidf = vectorizador.fit_transform(todas_las_respuestas)\n",
        "\n",
        "    # Devolver el vectorizador y la matriz TF-IDF resultante\n",
        "    return vectorizador, matriz_tfidf"
      ],
      "metadata": {
        "id": "sy6yfo-AX95c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from unidecode import unidecode\n",
        "\n",
        "def buscar_patrones_similares(entrada_usuario, vectorizador, matriz_tfidf, corpus):\n",
        "    # Preprocesar la entrada del usuario utilizando la función preprocesar_texto definida anteriormente\n",
        "    entrada_usuario_preprocesada = preprocesar_texto(entrada_usuario)\n",
        "\n",
        "    # Transformar la entrada del usuario en un vector TF-IDF utilizando el vectorizador\n",
        "    vector_entrada = vectorizador.transform([entrada_usuario_preprocesada])\n",
        "\n",
        "    # Calcular las puntuaciones de similitud coseno entre el vector de entrada y la matriz TF-IDF\n",
        "    puntuaciones_similitud = cosine_similarity(vector_entrada, matriz_tfidf)\n",
        "\n",
        "    # Obtener los índices de las respuestas más similares en orden descendente de similitud\n",
        "    indices_mas_similares = puntuaciones_similitud.argsort()[0, ::-1]\n",
        "\n",
        "    # Inicializar una lista para almacenar las respuestas relevantes\n",
        "    respuestas = []\n",
        "\n",
        "    # Iterar a través de los índices de respuestas más similares\n",
        "    for indice in indices_mas_similares:\n",
        "        similitud = puntuaciones_similitud[0, indice]\n",
        "        if similitud >= 0.2:\n",
        "            patrones = []\n",
        "\n",
        "            # Extender la lista de patrones con los patrones de todas las secciones en el corpus\n",
        "            for seccion in corpus.values():\n",
        "                patrones.extend(seccion['patrones'])\n",
        "\n",
        "            # Obtener el patrón similar basado en el índice actual\n",
        "            patron_similar = patrones[indice].lower()\n",
        "\n",
        "            # Convertir el patrón similar a una forma ASCII normalizada (quitar acentos)\n",
        "            patron_similar = unidecode(patron_similar)\n",
        "\n",
        "            # Buscar el patrón similar en las secciones del corpus y obtener las respuestas correspondientes\n",
        "            for clave, valor in corpus.items():\n",
        "                for patron in valor['patrones']:\n",
        "                    if unidecode(patron.lower()) == patron_similar:\n",
        "                        respuestas = valor['respuestas']\n",
        "                        break\n",
        "            break\n",
        "\n",
        "    return respuestas\n"
      ],
      "metadata": {
        "id": "NRnrnrIXYKvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "LqZE3RxikcUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SALUDOS_INPUTS = (\"hola\",\"buenas\", \"saludos\", \"qué tal\", \"hey\", \"buenos dias\", \"Buenos dias\", \"Buenas\", \"buenas\", \"hola, que tal?\", \"como estas?\")\n",
        "SALUDOS_OUTPUTS = [\"Hola\", \"Hola, ¿Cómo te puedo ayudar?\", \"Hola, encantado de hablar contigo\",\"¿Hola, en que puedo ayudarte?\"]\n",
        "\n",
        "def saludos(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in SALUDOS_INPUTS:\n",
        "      return random.choice(SALUDOS_OUTPUTS)"
      ],
      "metadata": {
        "id": "fT7tmYKDibvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"\\n=====================================================================================================================\")\n",
        "    print(\"\\n************************* ¡BIENVENIDO AL CHATBOT DE LA ESCUELA DE INGENIERÍA INFORMÁTICA! ***************************\")\n",
        "    print(\"************************************************** ¡YACHAYBOT! ******************************************************\")\n",
        "    print(\"\\n=====================================================================================================================\\n\")\n",
        "    print(\" Puedes escribir 'salir' en cualquier momento para terminar.\\n\")\n",
        "\n",
        "    vectorizador, matriz_tfidf = crear_vectores_tfidf(corpus)\n",
        "\n",
        "    while True:\n",
        "        entrada_usuario = input(\"+ Tú: \")\n",
        "        print()\n",
        "        r = saludos(entrada_usuario.lower())\n",
        "\n",
        "        if entrada_usuario.lower() in ['salir', 'adiós', 'chao', 'cerrar','adios','chau', 'hasta pronto','gracias', 'salir']:\n",
        "            print(\">> YachayBot: ¡Hasta luego!\")\n",
        "            break\n",
        "\n",
        "        respuestas = buscar_patrones_similares(entrada_usuario, vectorizador, matriz_tfidf, corpus)\n",
        "        if (r != None):\n",
        "          print(\">> YachayBot: \" + r)\n",
        "          print(\"\")\n",
        "        else:\n",
        "            if not respuestas:\n",
        "              print(\">> YachayBot: Lo siento, no entiendo tu pregunta.\\n\")\n",
        "            else:\n",
        "                for respuesta in respuestas:\n",
        "                    print(\">> YachayBot:\", respuesta)\n",
        "                print(\"\")\n",
        "\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkS34lM0R704",
        "outputId": "95d21dc7-b397-4a9a-d45b-05feb7f489b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================================================================================================\n",
            "\n",
            "************************* ¡BIENVENIDO AL CHATBOT DE LA ESCUELA DE INGENIERÍA INFORMÁTICA! ***************************\n",
            "************************************************** ¡YACHAYBOT! ******************************************************\n",
            "\n",
            "=====================================================================================================================\n",
            "\n",
            " Puedes escribir 'salir' en cualquier momento para terminar.\n",
            "\n",
            "+ Tú: Hola\n",
            "\n",
            ">> YachayBot: Hola\n",
            "\n",
            "+ Tú: que tesis estan insnritas\n",
            "\n",
            ">> YachayBot: - Implementación de un dataset para la evaluación de modelos de análisis de sentimientos en la clasificación de tweets\n",
            ">> YachayBot: - Aplicación para el reconocimiento de texto y transformación de voz, y desarrollo de un prototipo de impresora braille orientado a personas con discapacidad visual\n",
            ">> YachayBot: - Análisis comparativo de la performance de los descriptores Wavelet y Fourier, aplicado a la detección de anomalías en trayectorias\n",
            ">> YachayBot: - Generación automática de letras de canciones usando redes neuronales recurrentes para el quechua\n",
            ">> YachayBot: - Implementación de un sistema experto para el diagnóstico de las cardiopatías más comunes en adultos utilizando lógica difusa\n",
            ">> YachayBot: - Aplicación de visión artificial en la estimación del peso corporal del cuy\n",
            ">> YachayBot: - Implementación de la red de acceso a internet por fibra óptica para el desarrollo de las clases virtuales de la Institución Educativa Wiñaypaq de la comunidad campesina de Huandar del distrito de Pisac\n",
            ">> YachayBot: - Evaluación de arquitecturas Deep Learning para identificar anomalías por deficiencia de macronutrientes en Persea Americana variedad Hass\n",
            ">> YachayBot: - Implementación de técnicas de minería de texto para la clasificación de tickets de soporte en la oficina de Tecnologías de la Información de EGEMSA\n",
            ">> YachayBot: - Identificación de establecimientos comerciales no registrados en mapas digitales\n",
            ">> YachayBot: - Construcción de un extractor de características basado en modelos pre-entrenados de redes neuronales convolucionales, para la identificación de imágenes de pinturas coloniales de la ciudad del Cusco\n",
            ">> YachayBot: - Prototipo de un sistema portátil para el tratamiento preventivo de ulceraciones por presión del pie diabético\n",
            ">> YachayBot: - Evaluación de técnicas de Change detection utilizando imágenes satelitales Landsat 8 para identificar cambios en la superficie causados por la minería\n",
            ">> YachayBot: - Algoritmo para el problema data streaming clustering para conjuntos amorfos y con outliers\n",
            ">> YachayBot: - Construcción de un prototipo de sistema para clasificar enfermedades en las hojas de cafeto basado en visión computacional\n",
            ">> YachayBot: - Diseño y evaluación de un modelo basado en una red de cápsulas de matrices con em routing utilizando una red convolucional densa\n",
            ">> YachayBot: - Identificación de métodos de procesamiento digital de imágenes bacteriológicas microscópicas para el diagnóstico de tuberculosis\n",
            ">> YachayBot: - Análisis masivo de datos en twitter para identificación de opinión\n",
            ">> YachayBot: - Aplicación del método de agrupamiento lineal en inferencia de redes genéticas probabilísticas aplicado al Plasmodium Falciparum\n",
            ">> YachayBot: - Chatbot generativo en el idioma español utilizando la arquitectura de red neuronal Transformer\n",
            "\n",
            "+ Tú: cerrar\n",
            "\n",
            ">> YachayBot: ¡Hasta luego!\n"
          ]
        }
      ]
    }
  ]
}